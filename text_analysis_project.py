# -*- coding: utf-8 -*-
"""Text Analysis Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11aDfDvr-6--TX3kItEse0pRj4A9jzEoD

### NECCESARY IMPORTS
"""

# Importing necessary libraries

import pandas as pd

import requests
import html5lib
from bs4 import BeautifulSoup

from nltk.text import TokenSearcher
import nltk
nltk.download('punkt')

import string
nltk.download('stopwords')
from nltk.corpus import stopwords

from nltk.tokenize import sent_tokenize

nltk.download('opinion_lexicon')
from nltk.corpus import opinion_lexicon

"""## DATA EXTRACTION"""

# Load the xlsx file
df = pd.read_excel("/content/Input.xlsx")
display(df)

def data_extraction(URL,URL_ID):

  ''' 
  This function extracts the page content from the url and saves as a text file
  input : URL and URl ID
  output : text file
  '''

  headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',
  }
  # Getting page content from URL
  r = requests.get(URL, headers=headers)
  soup = BeautifulSoup(r.content, 'html5lib') 

  # Extracting title from content
  title = soup.find('h1', class_ ='entry-title').text

  # Extracting paragraph from content
  text = soup.find("div", class_ = "td-post-content").text

  # spliting text content and Removing empty rows
  text_content = [x for x in text.split("\n") if x != '']
  
  # merging title and content
  final = title + "\n" +" ".join(text_content)
  
  #saving the content in text file with url_id as file  name
  with open(f"{URL_ID}.txt", "w") as files:
    files.write(final)

# applying the data extraction function to all the rows in the input file
for index, row in df.iterrows():
    print(row['URL_ID'], row['URL'])
    data_extraction(row["URL"],row["URL_ID"])

"""## DATA ANALYSIS"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

cd drive/MyDrive/assignment

def get_necessary_inputs(filename):
  
  '''this function generates the important inputs required for the text analysis '''

  with open(filename, "r") as f:
    text = f.readlines()

  #generating tokens from the text
  tokens = nltk.word_tokenize(text[1])

  # Removing punctuations
  s = list(string.punctuation)
  punctuations_removed_tokens = [x for x in tokens if x not in s]

  # removing stop words
  stop_words = set(stopwords.words('english'))
  stop_words_removed_tokens = [x.lower() for x in punctuations_removed_tokens if x.lower() not in stop_words]

  # spliting words into sentences
  sentences = sent_tokenize(text[1])

  # generating positive and negative dictionaries
  pos_list = [x.lower() for x in set(opinion_lexicon.positive())]
  neg_list = [x.lower() for x in set(opinion_lexicon.negative())]

  return punctuations_removed_tokens, stop_words_removed_tokens, sentences, pos_list, neg_list

"""SENTIMENTAL ANALYSIS"""

# positive score and negative score

def get_positive_score(stop_words_removed_tokens):
  positive_score = 0
  negative_score = 0
  for each_word in stop_words_removed_tokens:

    # Assigning +1 for positive words
    if each_word.lower() in pos_list:
      positive_score +=1
      
    # Assigning -1 for negative words
    elif each_word.lower() in neg_list:
      negative_score +=1

  return positive_score, negative_score

# Polarity Score = (Positive Score â€“ Negative Score)/ ((Positive Score + Negative Score) + 0.000001)

def get_polarity_Score(positive_words,negative_words):
  # calculating polarity score
  Polarity_Score = (positive_words - negative_words) / ((positive_words + negative_words) + 0.000001)
  return Polarity_Score

# Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)

def get_Subjectivity_Score(positive_words,negative_words,stop_words_removed_tokens):
  # calculating subjective score
  Subjectivity_Score = (positive_words - negative_words) / ((len(stop_words_removed_tokens)) + 0.000001)
  return Subjectivity_Score

"""ANALYSIS OF READABILITY"""

# Average Sentence Length = the number of words / the number of sentences

def get_average_sentence_length(punctuations_removed_tokens,sentences):

  # calculating the length of tokens
  number_of_words = len(punctuations_removed_tokens)

  # calculating the length of sentences
  number_of_sentences = len(sentences)

  # calculating average sentence length
  average_sentence_length = number_of_words / number_of_sentences

  return average_sentence_length

# Percentage of Complex words = the number of complex words / the number of words 

def get_percentage_of_complex_words(punctuations_removed_tokens):
  number_of_complex_words = get_complex_word_count(punctuations_removed_tokens)

  # calculating the length of tokens
  total_number_of_words = len(punctuations_removed_tokens)

  # calculating the percentage of complex words
  percentage_of_complex_words = number_of_complex_words / total_number_of_words

  return percentage_of_complex_words

# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)

def get_fog_index(punctuations_removed_tokens,sentences):

  # calculating average sentence length 
  average_sentence_length = get_average_Number_of_Words_Per_Sentence(punctuations_removed_tokens,sentences)

  # calculating percentage of complex words 
  percentage_of_complex_words = get_percentage_of_complex_words(punctuations_removed_tokens)

  # calculating fog index  
  fog_index = 0.4 * average_sentence_length + percentage_of_complex_words

  return fog_index

"""AVERAGE NUMBER OF WORDS PER SENTENCE"""

# the total number of words / the total number of sentences

def get_average_Number_of_Words_Per_Sentence(punctuations_removed_tokens,sentences):

  # calculating the length of tokens
  total_number_of_words = len(punctuations_removed_tokens)

  # calculating the length of sentences
  total_number_of_sentences = len(sentences)
  
  # calculating average number of words per sentence
  average_Number_of_Words_Per_Sentence = total_number_of_words / total_number_of_sentences

  return average_Number_of_Words_Per_Sentence

"""COMPLEX WORD COUNT"""

def get_complex_word_count(punctuations_removed_tokens):

  # calculating complex word count using  tokens
  syllable_count = get_syllable_count_per_word(punctuations_removed_tokens)

  # returning the length of syllable count items
  return len([k for k,v in syllable_count.items() if v >= 2])

"""WORD COUNT"""

def get_total_cleaned_count(punctuations_removed_tokens):

  # calculating the length of stop words removed 
  return len(stop_words_removed_tokens)
  
# get_total_cleaned_count(punctuations_removed_tokens)

"""SYLLABLE COUNT PER WORD"""

def get_syllable_count_per_word(punctuations_removed_tokens):

  # initializing count 0 for all the words
  syllable_count = {x:0 for x in punctuations_removed_tokens}
  v = "aeiou"
  for each_word in punctuations_removed_tokens:

    # calculating number of vowels in the word
    syllable_count[each_word] = len([x for x in each_word if x.lower() in v])

  return syllable_count

"""PERSONAL PRONOUNS"""

def get_personal_pronouns(punctuations_removed_tokens): 
  a = ["I","we","my","ours","us"]
  count = 0
  for each in punctuations_removed_tokens:
    s = [x.lower() for x in a]
    if each.lower() in s:
      count+=1
  return count

"""AVERAGE WORD LENGTH"""

# Sum of the total number of characters in each word/Total number of words

def get_average_word_length(punctuations_removed_tokens):

  # calculating the length of  tokens
  total_number_of_words = len(punctuations_removed_tokens)

  # calculating the sum of length of tokens
  total_character_count = sum([len(x) for x in punctuations_removed_tokens])

  # calculating the average word length
  average_word_length = total_character_count / total_number_of_words

  return average_word_length

# Load the output xlsx file
output_data = pd.read_excel("/content/Output Data Structure.xlsx")
display(output_data)

for i,row in output_data.iterrows():
  '''applying all the fuctions to the output file'''
  try:
    print(f"{int(row['URL_ID'])}.txt")
    punctuations_removed_tokens, stop_words_removed_tokens, sentences, pos_list, neg_list = get_necessary_inputs(f"{int(row['URL_ID'])}.txt")
    output_data["POSITIVE SCORE"][i], output_data["NEGATIVE SCORE"][i] = get_positive_score(stop_words_removed_tokens)
    output_data["POLARITY SCORE"][i] = get_polarity_Score(output_data["POSITIVE SCORE"][i], output_data["NEGATIVE SCORE"][i])
    output_data["SUBJECTIVITY SCORE"][i] = get_Subjectivity_Score(output_data["POSITIVE SCORE"][i], output_data["NEGATIVE SCORE"][i],stop_words_removed_tokens)
    output_data["AVG SENTENCE LENGTH"][i] = get_average_sentence_length(punctuations_removed_tokens,sentences)
    output_data["PERCENTAGE OF COMPLEX WORDS"][i] = get_percentage_of_complex_words(punctuations_removed_tokens)
    output_data["FOG INDEX"][i] = get_fog_index(punctuations_removed_tokens,sentences)
    output_data["AVG NUMBER OF WORDS PER SENTENCE"][i] = get_average_Number_of_Words_Per_Sentence(punctuations_removed_tokens,sentences)
    output_data["COMPLEX WORD COUNT"][i] = get_complex_word_count(punctuations_removed_tokens)
    output_data["WORD COUNT"][i] = get_total_cleaned_count(punctuations_removed_tokens)
    output_data["SYLLABLE PER WORD"][i] = str(get_syllable_count_per_word(punctuations_removed_tokens))
    output_data["PERSONAL PRONOUNS"][i] = get_personal_pronouns(punctuations_removed_tokens)
    output_data["AVG WORD LENGTH"][i] = get_average_word_length(punctuations_removed_tokens)

  except Exception as error:
    print(error)

output_data.head()

#exporting the output generated to excel
output_data.to_excel("jayasree_output.xlsx")

